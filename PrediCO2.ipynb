{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f0ec4-552d-4614-88a1-acd97f035649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Dropout,MultiHeadAttention,LayerNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.losses import MeanSquaredError,MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from time import perf_counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b071f",
   "metadata": {},
   "source": [
    "## you could delete this line here (os one)\n",
    "- i needed it cause im using wsl for tensorflow, but in normal windows, its possible to remove it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/tf-acno-projects/Project-Data-Mining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_forecasting_splits(Datafile,time_steps):\n",
    "    X,y = list(),list()\n",
    "    for start in range(len(Datafile)):\n",
    "        end = start+time_steps \n",
    "        if end>len(Datafile)-1:\n",
    "            break\n",
    "        X.append(Datafile.iloc[start:end].values)\n",
    "        y.append(Datafile.iloc[end][\"CO2 Emission\"])\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_converter(DataFile):\n",
    "    unique_months = DataFile['Month'].unique()\n",
    "    months_dict = {\n",
    "        month:idx+1 for idx,month in enumerate(unique_months)\n",
    "    }\n",
    "    DataFile['Month'] = DataFile['Month'].map(months_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc50550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_architecture(input_shape,lr):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    hidden_layer = LSTM(16)(input_layer)\n",
    "    #hidden_layer = Dense(8,activation='relu',kernel_regularizer=l2(0.0001))(hidden_layer)\n",
    "    output_layer = Dense(1,activation='linear')(hidden_layer)\n",
    "\n",
    "    lstm_model = Model(input_layer,output_layer)\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=lr),loss=MeanSquaredError(),metrics=[MeanAbsoluteError()])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227e9c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(sequence_length,d_model,n=10000):\n",
    "    \"\"\"\n",
    "    - d_model: the dimension of our input ( output of the embedding space )\n",
    "    - sequence_length: the length of our sequence for example we have 3 features then its 3\n",
    "    \"\"\"\n",
    "    PosEnc = np.zeros((sequence_length,d_model))\n",
    "    indices = np.arange(int(d_model/2))\n",
    "    positions = np.arange((sequence_length))\n",
    "    \n",
    "    for position in positions:\n",
    "        for index in indices:\n",
    "            denomenator = np.power(n,2*index/d_model)\n",
    "            PosEnc[position,2*index] = np.sin(position/denomenator)\n",
    "            PosEnc[position,2*index+1] = np.cos(position/denomenator)\n",
    "\n",
    "    pos_enc_tensor = tf.constant(PosEnc, dtype=tf.float32)\n",
    "    pos_enc_tensor = tf.reshape(pos_enc_tensor, (1, sequence_length, d_model))\n",
    "    return pos_enc_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debdefbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_embedding_positioning(sequence_length,d_model,input_layer):\n",
    "    \"\"\"\n",
    "    - We use **the embedding** which is a way to convert raw input into a high-dimensional vector (d_model in this case).\n",
    "        so our embedding vector shape is **(batch_size,time,features_embedding)**\n",
    "    - **Positional encoding** is added to this embedding vector so the model knows about the position of each token or feature.\n",
    "    \"\"\"\n",
    "    x_embedded = Dense(d_model)(input_layer)\n",
    "    pos_enc = positional_encoding(sequence_length,d_model)\n",
    "    return x_embedded + pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794fb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(x,d_model,num_heads,key_dim):\n",
    "    attention_layer = MultiHeadAttention(num_heads=num_heads,key_dim=key_dim)(x,x)\n",
    "    add_norm1 = LayerNormalization(epsilon=1e-6)(x+attention_layer)\n",
    "    \n",
    "    feed_forward_input = Dense(d_model*2,activation='relu')(add_norm1)\n",
    "    feed_forward_output = Dense(d_model)(feed_forward_input)\n",
    "    add_norm2 = LayerNormalization(epsilon=1e-6)(add_norm1+feed_forward_output)\n",
    "    \n",
    "    return add_norm2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc07bb9",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "  <img src=\"static/encoder_transformer.png\" width=\"300\" height=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c066abb6",
   "metadata": {},
   "source": [
    "- The image above taken from the Transformer Encoder Architecture set by <a href='https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf' >'Attention is all you need'</a> Paper\n",
    "- We didnt used the second part (the decoder) cause we want to predict for each input a single output and not as the decoder does by giving a whole batch of outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc78b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_architecture(input_shape,lr,sequence_length,d_model,num_heads,key_dim):   \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    x_positioned_embedded = transformer_embedding_positioning(sequence_length,d_model,input_layer) \n",
    "    x1 = transformer_encoder(x_positioned_embedded,d_model,num_heads,key_dim)\n",
    "    x2 = transformer_encoder(x1,d_model,num_heads,key_dim)\n",
    "    output_layer = Dense(1)(x2)\n",
    "    \n",
    "    transformer = Model(input_layer,output_layer)\n",
    "    transformer.compile(optimizer=Adam(learning_rate=lr),loss=MeanSquaredError(),metrics=[MeanAbsoluteError()])\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99e569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def models_perf_metrics_saving(perf_df_path,metrics_df_path,new_perf_df,new_metrics_df):\n",
    "    if os.path.exists(os.path.join(os.getcwd(), perf_df_path)):   \n",
    "        performance_df_old = pd.read_csv(perf_df_path)\n",
    "        performance_df_old = pd.concat([performance_df_old,new_perf_df],ignore_index=True)\n",
    "        metrics_df_old = pd.read_csv(metrics_df_path)\n",
    "        metrics_df_old = pd.concat([metrics_df_old,new_metrics_df],ignore_index=True)\n",
    "        \n",
    "        performance_df_old.to_csv(perf_df_path,index=False)\n",
    "        metrics_df_old.to_csv(metrics_df_path,index=False)\n",
    "        print(f'Concatenation to the old {perf_df_path}, {metrics_df_path} csv files')\n",
    "    else:\n",
    "        new_perf_df.to_csv(perf_df_path)\n",
    "        new_metrics_df.to_csv(metrics_df_path)\n",
    "        print(f'Creation of new {perf_df_path}, {metrics_df_path} csv files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39daf82a-f82a-45c0-a62c-e820fde8b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(\"Emission.csv\")\n",
    "DataFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c1467-6b63-40db-ad7c-9c75f8297757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DataFile.isnull().sum())\n",
    "print(DataFile.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d0dc9-ce40-4e53-b339-6b6cf108aac8",
   "metadata": {},
   "source": [
    "Alright, there are no null values and no duplicates but there is something wrong with the \"Year-Month\" column, it's better to split it into two and convert them to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf323fe-847b-4303-926a-454baf775a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile[['Year', 'Month']] = DataFile['Year-Month'].str.split('-', expand=True)\n",
    "\n",
    "DataFile.drop(columns=['Year-Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_converter(DataFile)\n",
    "\n",
    "for col in DataFile.columns:\n",
    "    DataFile[col] = pd.to_numeric(DataFile[col],errors='coerce')\n",
    "print(DataFile)\n",
    "print(DataFile.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset Min : {DataFile[\"CO2 Emission\"].min()}')\n",
    "print(f'Dataset Max : {DataFile[\"CO2 Emission\"].max()}')\n",
    "print(f'Dataset Mean : {DataFile[\"CO2 Emission\"].mean()}')\n",
    "print(f'Dataset STD : {DataFile[\"CO2 Emission\"].std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26029c1f",
   "metadata": {},
   "source": [
    "this is a note that we should scale the data later on for our models so we could detect overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c41f3b-0ffb-4a21-ba5b-8f894b0ed742",
   "metadata": {},
   "source": [
    "Now we need to perform visual analysis on our dataset, but first we need to create a csv of our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690acd5-0369-49cf-b1ba-1fef274c93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile.to_csv(\"New Emission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe38774-4f7d-494c-bd52-8fb179be787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(\"New Emission.csv\")\n",
    "\n",
    "DataFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9019b6-bab1-49cd-94a6-8a9a7abbb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(DataFile[\"Year\"], DataFile[\"CO2 Emission\"], marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission Over the Years\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee544538-e546-4aae-97a5-c26c8014acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=DataFile[\"Month\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Month\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48417bf-3df5-41db-85cd-94d73c8efc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=DataFile[\"Year\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Year\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06e71d-27d9-4ad8-87a8-915ffe21ac25",
   "metadata": {},
   "source": [
    "It's kind of a complex figure so we will group the years into ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b68c8-bfbe-494a-92a0-f9ff70464ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = DataFile[\"Year\"].min()\n",
    "max_year = DataFile[\"Year\"].max()\n",
    "\n",
    "print(min_year)\n",
    "print(max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cc83e-dbd7-4483-aa0b-1370dc69ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015]\n",
    "\n",
    "labels = [\"1971-1975\", \"1976-1980\", \"1981-1985\", \"1986-1990\", \"1991-1995\", \"1996-2000\", \"2001-2005\", \"2006-2010\", \"2011-2015\"]\n",
    "\n",
    "DataFile[\"Year Range\"] = pd.cut(DataFile[\"Year\"], bins=bins, labels=labels, right=True)\n",
    "print(DataFile[[\"Year\", \"Year Range\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077229-454a-42e6-a1f9-7d5fbbec345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))  # Increase width\n",
    "sns.barplot(x=DataFile[\"Year Range\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "plt.xlabel(\"Year Range\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Year\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate labels for better spacing\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3900c72-e2b4-4024-b223-e6ee6e974516",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile.drop(columns=['Year Range'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea0655",
   "metadata": {},
   "source": [
    "## LSTM AND TRANSFORMERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1d2b08ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 3\n",
    "INPUT_SHAPE = (TIME_STEP,3) # (time_step, features)\n",
    "LR = 0.001\n",
    "EPOCHS = 80\n",
    "N_SPLITS = 3\n",
    "CALLBACK = [\n",
    "    ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.6,\n",
    "    patience=10,\n",
    "    min_delta=0.0001,\n",
    "    min_lr=1e-6,\n",
    "    ),]\n",
    "#EarlyStopping(monitor=\"val_loss\",patience=20,min_delta=0.0001,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "43f632a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = X_y_forecasting_splits(DataFile,TIME_STEP)\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fe593",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "folds_histories = []\n",
    "folds_metrics = []\n",
    "lstm_perf_filename = 'lstm_performance.csv'\n",
    "lstm_metrics_filename = 'lstm_metrics.csv'\n",
    "\n",
    "time_series_split_folds = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "for fold ,(training_idx, validation_idx) in enumerate(time_series_split_folds.split(X,y)):\n",
    "    lstm_model = lstm_architecture(INPUT_SHAPE,LR)\n",
    "\n",
    "    X_train_cv = tf.convert_to_tensor(X[training_idx], dtype=tf.float32)\n",
    "    X_val_cv = tf.convert_to_tensor(X[validation_idx], dtype=tf.float32)\n",
    "    y_train_cv = tf.convert_to_tensor(y_scaled[training_idx], dtype=tf.float32)\n",
    "    y_val_cv = tf.convert_to_tensor(y_scaled[validation_idx], dtype=tf.float32)\n",
    "    \n",
    "    train_start = perf_counter()\n",
    "    history = lstm_model.fit(X_train_cv,y_train_cv,epochs=EPOCHS,validation_data=(X_val_cv,y_val_cv),callbacks=CALLBACK,verbose=0)\n",
    "    train_end = perf_counter()\n",
    "    \n",
    "    val_loss, val_mae = lstm_model.evaluate(X_val_cv, y_val_cv,verbose=0)\n",
    "    \n",
    "    y_val_preds = lstm_model.predict(X_val_cv)\n",
    "    # Reshape predictions\n",
    "    y_val_preds_reshaped = y_val_preds.reshape(-1, 1) # from (dim1,1) to (dim1,)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_preds_original = scaler.inverse_transform(y_val_preds_reshaped).flatten() # to get the original co2 values not between 0-1\n",
    "\n",
    "    # Reshape validation data\n",
    "    y_val_reshaped = y_val_cv.numpy().reshape(-1, 1) # from (dim1,1) to (dim1,)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_original = scaler.inverse_transform(y_val_reshaped).flatten() # to get the original co2 values not between 0-1\n",
    "    \n",
    "    # Calculating the metrics :\n",
    "    y_mean = np.mean(y_val_original)\n",
    "    mae_scaled_calculated = np.mean(np.abs(y_val_preds_reshaped - y_val_reshaped))\n",
    "    mae_calculated = np.mean(np.abs(y_val_preds_original - y_val_original))\n",
    "    mape_calculated = np.mean(np.abs((y_val_preds_original - y_val_original) / y_val_original)) # mape = 1/n * sum( abs(y_real - y_predicted) )\n",
    "    \n",
    "    sce = np.sum((y_val_preds_original - y_mean) ** 2)\n",
    "    sst = np.sum((y_val_original - y_mean) ** 2)\n",
    "    r2_calculated = sce / sst # r2 = SCE / SST\n",
    "    \n",
    "    fold_history = history.history.copy()\n",
    "    fold_history['fold'] = fold\n",
    "    folds_metrics.append({\n",
    "        'fold':fold,\n",
    "        'training_time':train_end-train_start,\n",
    "        'mae_scaled_calculated':mae_scaled_calculated,\n",
    "        'mae_calculated':mae_calculated,\n",
    "        'mape_calculated':mape_calculated,\n",
    "        'r2_calculated':r2_calculated\n",
    "    })\n",
    "    folds_histories.append(fold_history)\n",
    "    \n",
    "performance_df = pd.concat([pd.DataFrame(h) for h in folds_histories])\n",
    "metrics_df = pd.DataFrame(folds_metrics)\n",
    "performance_df.to_csv(lstm_perf_filename)\n",
    "metrics_df.to_csv(lstm_metrics_filename)\n",
    "#models_perf_metrics_saving(lstm_perf_filename,lstm_met_filename,performance_df,metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c65703",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6d06d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 64\n",
    "SEQ_LEN = TIME_STEP\n",
    "\n",
    "NUM_HEADS = 4\n",
    "KEY_DIM = D_MODEL//NUM_HEADS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6e5920ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 13ms/step\n",
      "4/4 [==============================] - 0s 9ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "folds_histories = []\n",
    "folds_metrics = []\n",
    "transformer_perf_filename = 'transformer_performance.csv'\n",
    "transformer_metrics_filename = 'transformer_metrics.csv'\n",
    "\n",
    "time_series_split_folds = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "performance = []\n",
    "for fold ,(training_idx, validation_idx) in enumerate(time_series_split_folds.split(X,y)):\n",
    "    transformer_model = transformer_architecture(INPUT_SHAPE,LR,SEQ_LEN,D_MODEL,NUM_HEADS,KEY_DIM)\n",
    "\n",
    "    X_train_cv = tf.convert_to_tensor(X[training_idx], dtype=tf.float32)\n",
    "    X_val_cv = tf.convert_to_tensor(X[validation_idx], dtype=tf.float32)\n",
    "    y_train_cv = tf.convert_to_tensor(y_scaled[training_idx], dtype=tf.float32)\n",
    "    y_val_cv = tf.convert_to_tensor(y_scaled[validation_idx], dtype=tf.float32)\n",
    "    \n",
    "    train_start = perf_counter()\n",
    "    history = transformer_model.fit(X_train_cv,y_train_cv,epochs=EPOCHS,validation_data=(X_val_cv,y_val_cv),callbacks=CALLBACK,verbose=0)\n",
    "    train_end = perf_counter()\n",
    "    \n",
    "    val_loss, val_mae = transformer_model.evaluate(X_val_cv, y_val_cv,verbose=0)\n",
    "    \n",
    "    y_val_preds = transformer_model.predict(X_val_cv)\n",
    "    y_val_preds = y_val_preds[:, -1, :]  # The last predict gives 3 predictions for each input , thus we only take the last one with -1\n",
    "    \n",
    "    # Reshape predictions\n",
    "    y_val_preds_reshaped = y_val_preds.reshape(-1, 1) # from (dim1,1) to (dim1,)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_preds_original = scaler.inverse_transform(y_val_preds_reshaped).flatten() # to get the original co2 values not between 0-1\n",
    "\n",
    "    # Reshape validation data\n",
    "    y_val_reshaped = y_val_cv.numpy().reshape(-1, 1) # from (dim1,1) to (dim1,)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_original = scaler.inverse_transform(y_val_reshaped).flatten() # to get the original co2 values not between 0-1\n",
    "    \n",
    "    # Calculating the metrics :\n",
    "    y_mean = np.mean(y_val_original)\n",
    "    mae_scaled_calculated = np.mean(np.abs(y_val_preds_reshaped - y_val_reshaped))\n",
    "    mae_calculated = np.mean(np.abs(y_val_preds_original - y_val_original))\n",
    "    mape_calculated = np.mean(np.abs((y_val_preds_original - y_val_original) / y_val_original)) # mape = 1/n * sum( abs(y_real - y_predicted) )\n",
    "    \n",
    "    sce = np.sum((y_val_preds_original - y_mean) ** 2)\n",
    "    sst = np.sum((y_val_original - y_mean) ** 2)\n",
    "    r2_calculated = sce / sst # r2 = SCE / SST\n",
    "    \n",
    "    fold_history = history.history.copy()\n",
    "    fold_history['fold'] = fold\n",
    "    folds_metrics.append({\n",
    "        'fold':fold,\n",
    "        'training_time':train_end-train_start,\n",
    "        'mae_scaled_calculated':mae_scaled_calculated,\n",
    "        'mae_calculated':mae_calculated,\n",
    "        'mape_calculated':mape_calculated,\n",
    "        'r2_calculated':r2_calculated\n",
    "    })\n",
    "    folds_histories.append(fold_history)\n",
    "    \n",
    "performance_df = pd.concat([pd.DataFrame(h) for h in folds_histories])\n",
    "metrics_df = pd.DataFrame(folds_metrics)\n",
    "performance_df.to_csv(transformer_perf_filename)\n",
    "metrics_df.to_csv(transformer_metrics_filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9f0ec4-552d-4614-88a1-acd97f035649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,Dropout,MultiHeadAttention,LayerNormalization\n",
    "from tensorflow.keras.losses import MeanSquaredError,MeanAbsoluteError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau,EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1b071f",
   "metadata": {},
   "source": [
    "## you could delete this line here (os one)\n",
    "- i needed it cause im using wsl for tensorflow, but in normal windows, its possible to remove it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c423fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/tf-acno-projects/Project-Data-Mining')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25d3382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X_y_forecasting_splits(Datafile,time_steps):\n",
    "    X,y = list(),list()\n",
    "    for start in range(len(Datafile)):\n",
    "        end = start+time_steps \n",
    "        if end>len(Datafile)-1:\n",
    "            break\n",
    "        X.append(Datafile.iloc[start:end].values)\n",
    "        y.append(Datafile.iloc[end][\"CO2 Emission\"])\n",
    "    return np.array(X),np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d14c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def months_converter(DataFile):\n",
    "    unique_months = DataFile['Month'].unique()\n",
    "    months_dict = {\n",
    "        month:idx+1 for idx,month in enumerate(unique_months)\n",
    "    }\n",
    "    DataFile['Month'] = DataFile['Month'].map(months_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc50550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_architecture(INPUT_SHAPE,LR):\n",
    "    input_layer = Input(shape=INPUT_SHAPE)\n",
    "    hidden_layer = LSTM(64)(input_layer)\n",
    "    hidden_layer = Dense(32,activation='relu')(hidden_layer)\n",
    "    hidden_layer = Dense(16,activation='relu')(hidden_layer)\n",
    "    output_layer = Dense(1,activation='linear')(hidden_layer)\n",
    "\n",
    "    lstm_model = Model(input_layer,output_layer)\n",
    "    #lstm_model.summary()\n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=LR),loss=MeanSquaredError(),metrics=[MeanAbsoluteError()])\n",
    "    return lstm_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39daf82a-f82a-45c0-a62c-e820fde8b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(\"Emission.csv\")\n",
    "DataFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3c1467-6b63-40db-ad7c-9c75f8297757",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DataFile.isnull().sum())\n",
    "print(DataFile.duplicated().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269d0dc9-ce40-4e53-b339-6b6cf108aac8",
   "metadata": {},
   "source": [
    "Alright, there are no null values and no duplicates but there is something wrong with the \"Year-Month\" column, it's better to split it into two and convert them to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf323fe-847b-4303-926a-454baf775a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile[['Year', 'Month']] = DataFile['Year-Month'].str.split('-', expand=True)\n",
    "\n",
    "DataFile.drop(columns=['Year-Month'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0154ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "months_converter(DataFile)\n",
    "\n",
    "for col in DataFile.columns:\n",
    "    DataFile[col] = pd.to_numeric(DataFile[col],errors='coerce')\n",
    "print(DataFile)\n",
    "print(DataFile.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2830b789",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Dataset Min : {DataFile[\"CO2 Emission\"].min()}')\n",
    "print(f'Dataset Max : {DataFile[\"CO2 Emission\"].max()}')\n",
    "print(f'Dataset Mean : {DataFile[\"CO2 Emission\"].mean()}')\n",
    "print(f'Dataset STD : {DataFile[\"CO2 Emission\"].std()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26029c1f",
   "metadata": {},
   "source": [
    "this is a note that we should scale the data later on for our models so we could detect overfitting or underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c41f3b-0ffb-4a21-ba5b-8f894b0ed742",
   "metadata": {},
   "source": [
    "Now we need to perform visual analysis on our dataset, but first we need to create a csv of our new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3690acd5-0369-49cf-b1ba-1fef274c93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile.to_csv(\"New Emission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe38774-4f7d-494c-bd52-8fb179be787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile = pd.read_csv(\"New Emission.csv\")\n",
    "\n",
    "DataFile.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a20a4d2-efbc-413a-a2b9-bdadb07d004c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DataFile.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9019b6-bab1-49cd-94a6-8a9a7abbb43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(DataFile[\"Year\"], DataFile[\"CO2 Emission\"], marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission Over the Years\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee544538-e546-4aae-97a5-c26c8014acf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=DataFile[\"Month\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Month\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48417bf-3df5-41db-85cd-94d73c8efc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "sns.barplot(x=DataFile[\"Year\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "# Labels and Title\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Year\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06e71d-27d9-4ad8-87a8-915ffe21ac25",
   "metadata": {},
   "source": [
    "It's kind of a complex figure so we will group the years into ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809b68c8-bfbe-494a-92a0-f9ff70464ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_year = DataFile[\"Year\"].min()\n",
    "max_year = DataFile[\"Year\"].max()\n",
    "\n",
    "print(min_year)\n",
    "print(max_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57cc83e-dbd7-4483-aa0b-1370dc69ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [1970, 1975, 1980, 1985, 1990, 1995, 2000, 2005, 2010, 2015]\n",
    "\n",
    "labels = [\"1971-1975\", \"1976-1980\", \"1981-1985\", \"1986-1990\", \"1991-1995\", \"1996-2000\", \"2001-2005\", \"2006-2010\", \"2011-2015\"]\n",
    "\n",
    "DataFile[\"Year Range\"] = pd.cut(DataFile[\"Year\"], bins=bins, labels=labels, right=True)\n",
    "print(DataFile[[\"Year\", \"Year Range\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2077229-454a-42e6-a1f9-7d5fbbec345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))  # Increase width\n",
    "sns.barplot(x=DataFile[\"Year Range\"], y=DataFile[\"CO2 Emission\"], palette=\"coolwarm\")\n",
    "\n",
    "plt.xlabel(\"Year Range\")\n",
    "plt.ylabel(\"CO2 Emission (ppm)\")\n",
    "plt.title(\"CO2 Emission by Year\")\n",
    "\n",
    "plt.xticks(rotation=45, ha=\"right\")  # Rotate labels for better spacing\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3900c72-e2b4-4024-b223-e6ee6e974516",
   "metadata": {},
   "outputs": [],
   "source": [
    "DataFile.drop(columns=['Year Range'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daea0655",
   "metadata": {},
   "source": [
    "## LSTM AND TRANSFORMERS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87e1dd3",
   "metadata": {},
   "source": [
    "We have 486 rows so :\n",
    "- train 80% = int(len(DataFile)*0.8)+1 => 389\n",
    "- test 10% =  int(len(DataFile)*0.1) => 48\n",
    "- validation 10% = int(len(DataFile)*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f632a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 3\n",
    "X,y = X_y_forecasting_splits(DataFile,TIME_STEP)\n",
    "scaler = MinMaxScaler()\n",
    "y_scaled = scaler.fit_transform(y.reshape(-1,1)).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c24b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SHAPE = (TIME_STEP,3)\n",
    "LR = 0.1\n",
    "EPOCHS = 80\n",
    "N_SPLITS = 3\n",
    "CALLBACK = [\n",
    "    ReduceLROnPlateau(\n",
    "    monitor=\"val_loss\",\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    min_delta=0.0005,\n",
    "    min_lr=1e-6,\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=20,\n",
    "    min_delta=0.0001,\n",
    "    )]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369826a-921f-4e3a-821f-9d51f3569422",
   "metadata": {},
   "source": [
    "train_size = int(len(DataFile)*0.8) + 1 \n",
    "test_val_size = int(len(DataFile)*0.1)\n",
    "\n",
    "X_train,y_train = X[:train_size],y[:train_size]\n",
    "X_test,y_test= X[train_size:train_size+test_val_size],y[train_size:train_size+test_val_size]\n",
    "X_val,y_val = X[train_size+test_val_size:],y[train_size+test_val_size:]\n",
    "\n",
    "print(f'train size is : {train_size}, test val size is : {test_val_size}')\n",
    "print(f'train : {X_train.shape} , {y_train.shape}')\n",
    "print(f'test : {X_test.shape} , {y_test.shape}')\n",
    "print(f'val : {X_val.shape} , {y_val.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54fe593",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b7ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_split_folds = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "performance = []\n",
    "for fold ,(training_idx, validation_idx) in enumerate(time_series_split_folds.split(X,y)):\n",
    "    lstm_model = lstm_architecture(INPUT_SHAPE,LR)\n",
    "\n",
    "    X_train_cv = tf.convert_to_tensor(X[training_idx], dtype=tf.float32)\n",
    "    X_val_cv = tf.convert_to_tensor(X[validation_idx], dtype=tf.float32)\n",
    "    y_train_cv = tf.convert_to_tensor(y_scaled[training_idx], dtype=tf.float32)\n",
    "    y_val_cv = tf.convert_to_tensor(y_scaled[validation_idx], dtype=tf.float32)\n",
    "    \n",
    "    lstm_model.fit(X_train_cv,y_train_cv,epochs=EPOCHS,validation_data=(X_val_cv,y_val_cv),callbacks=CALLBACK,verbose=1)\n",
    "    val_loss, val_mae = lstm_model.evaluate(X_val_cv, y_val_cv,verbose=0)\n",
    "    \n",
    "    y_val_preds = lstm_model.predict(X_val_cv)\n",
    "    y_val_preds = scaler.inverse_transform(y_val_preds).flatten()\n",
    "    y_val_original = scaler.inverse_transform(y_val_cv.numpy().reshape(-1,1)).flatten()\n",
    "    \n",
    "    mae_original = np.mean(np.abs(y_val_preds - y_val_original))\n",
    "    \n",
    "    performance.append({\n",
    "    \"fold\": fold,\n",
    "    \"val_loss\": f'{val_loss:.4f}',\n",
    "    \"val_mae_scaled\": f'{val_mae:.4f}',\n",
    "    \"val_mae_original\": f'{mae_original:.4f}',\n",
    "    })    \n",
    "    \n",
    "    print(f'Fold {fold} , val_loss is : {val_loss:.2f}, MAE scaled is : {val_mae:.2f}, MAE original is : {mae_original:.2f}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51386a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = pd.DataFrame(performance)\n",
    "print(performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f270ba6",
   "metadata": {},
   "source": [
    "for the lstm model , thats the max we could have"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c65703",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "b3a7b76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(sequence_length,d_model,n=10000):\n",
    "    \"\"\"\n",
    "    d_model: the dimension of our input ( output of the embedding space )\n",
    "    sequence_length: the length of our sequence for example we have 3 features then its 3\n",
    "    \"\"\"\n",
    "    PosEnc = np.zeros((sequence_length,d_model))\n",
    "    indices = np.arange(int(d_model/2))\n",
    "    positions = np.arange((sequence_length))\n",
    "    \n",
    "    for position in positions:\n",
    "        for index in indices:\n",
    "            denomenator = np.power(n,2*index/d_model)\n",
    "            PosEnc[position,2*index] = np.sin(position/denomenator)\n",
    "            PosEnc[position,2*index+1] = np.cos(position/denomenator)\n",
    "\n",
    "    pos_enc_tensor = tf.constant(PosEnc, dtype=tf.float32)\n",
    "    pos_enc_tensor = tf.reshape(pos_enc_tensor, (1, sequence_length, d_model))\n",
    "    return pos_enc_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6d06d7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_MODEL = 64\n",
    "SEQ_LEN = 3\n",
    "N_SIZE = 10000\n",
    "\n",
    "NUM_HEADS = 8\n",
    "KEY_DIM = D_MODEL//NUM_HEADS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb8ae4d",
   "metadata": {},
   "source": [
    "- We use **the embedding** which is a way to convert raw input into a high-dimensional vector (d_model in this case).\n",
    "so our embedding vector shape is **(batch_size,time,features_embedding)**\n",
    "- **Positional encoding** is added to this embedding vector so the model knows about the position of each token or feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_embedding_positioning(input_layer):\n",
    "    x_embedded = Dense(D_MODEL)(input_layer)\n",
    "    pos_enc = positional_encoding(SEQ_LEN,D_MODEL)\n",
    "    return x_embedded + pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "42ead8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(x):\n",
    "    attention_layer = MultiHeadAttention(num_heads=NUM_HEADS,key_dim=KEY_DIM)(x,x)\n",
    "    add_norm1 = LayerNormalization(epsilon=1e-6)(x+attention_layer)\n",
    "    \n",
    "    feed_forward_input = Dense(D_MODEL*2,activation='relu')(add_norm1)\n",
    "    feed_forward_output = Dense(D_MODEL)(feed_forward_input)\n",
    "    add_norm2 = LayerNormalization(epsilon=1e-6)(add_norm1+feed_forward_output)\n",
    "    \n",
    "    return add_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "009e58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_architecture():   \n",
    "    input_layer = Input(shape=INPUT_SHAPE)\n",
    "    x_positioned_embedded = transformer_embedding_positioning(input_layer) \n",
    "    x1 = transformer_encoder(x_positioned_embedded)\n",
    "    x2 = transformer_encoder(x1)\n",
    "    output_layer = Dense(1)(x2)\n",
    "    \n",
    "    transformer = Model(input_layer,output_layer)\n",
    "    transformer.compile(optimizer=Adam(learning_rate=LR),loss=MeanSquaredError(),metrics=[MeanAbsoluteError()])\n",
    "    return transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5920ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEQ_LEN: 3, D_MODEL: 64, INPUT_DIM: (3, 3)\n",
      "Embedded shape: (None, 3, 64)\n",
      "Positional encoding shape: (1, 3, 64)\n",
      "Epoch 1/80\n",
      "4/4 [==============================] - 7s 234ms/step - loss: 55.4555 - mean_absolute_error: 4.4695 - val_loss: 0.3104 - val_mean_absolute_error: 0.5531 - lr: 0.1000\n",
      "Epoch 2/80\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.9816 - mean_absolute_error: 0.9311 - val_loss: 0.5383 - val_mean_absolute_error: 0.7233 - lr: 0.1000\n",
      "Epoch 3/80\n",
      "4/4 [==============================] - 0s 76ms/step - loss: 0.4569 - mean_absolute_error: 0.6128 - val_loss: 1.3680 - val_mean_absolute_error: 1.1700 - lr: 0.1000\n",
      "Epoch 4/80\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.5364 - mean_absolute_error: 0.6359 - val_loss: 0.0116 - val_mean_absolute_error: 0.0811 - lr: 0.1000\n",
      "Epoch 5/80\n",
      "4/4 [==============================] - 0s 59ms/step - loss: 0.1900 - mean_absolute_error: 0.4130 - val_loss: 0.0442 - val_mean_absolute_error: 0.1833 - lr: 0.1000\n",
      "Epoch 6/80\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0654 - mean_absolute_error: 0.2260 - val_loss: 0.3505 - val_mean_absolute_error: 0.5885 - lr: 0.1000\n",
      "Epoch 7/80\n",
      "4/4 [==============================] - 0s 64ms/step - loss: 0.1288 - mean_absolute_error: 0.3207 - val_loss: 0.0143 - val_mean_absolute_error: 0.1044 - lr: 0.1000\n",
      "Epoch 8/80\n",
      "4/4 [==============================] - 0s 56ms/step - loss: 0.0696 - mean_absolute_error: 0.2378 - val_loss: 0.0119 - val_mean_absolute_error: 0.0817 - lr: 0.1000\n",
      "Epoch 9/80\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.0267 - mean_absolute_error: 0.1423 - val_loss: 0.1654 - val_mean_absolute_error: 0.3997 - lr: 0.1000\n",
      "Epoch 10/80\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0379 - mean_absolute_error: 0.1634 - val_loss: 0.0098 - val_mean_absolute_error: 0.0853 - lr: 0.1000\n",
      "Epoch 11/80\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.0297 - mean_absolute_error: 0.1504 - val_loss: 0.0249 - val_mean_absolute_error: 0.1395 - lr: 0.1000\n",
      "Epoch 12/80\n",
      "4/4 [==============================] - 0s 55ms/step - loss: 0.0146 - mean_absolute_error: 0.1006 - val_loss: 0.0734 - val_mean_absolute_error: 0.2579 - lr: 0.1000\n",
      "Epoch 13/80\n",
      "4/4 [==============================] - 0s 68ms/step - loss: 0.0128 - mean_absolute_error: 0.0928 - val_loss: 0.0102 - val_mean_absolute_error: 0.0877 - lr: 0.1000\n",
      "Epoch 14/80\n",
      "4/4 [==============================] - 0s 63ms/step - loss: 0.0115 - mean_absolute_error: 0.0882 - val_loss: 0.0598 - val_mean_absolute_error: 0.2304 - lr: 0.1000\n",
      "Epoch 15/80\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0104 - mean_absolute_error: 0.0838 - val_loss: 0.0217 - val_mean_absolute_error: 0.1292 - lr: 0.1000\n",
      "Epoch 16/80\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0078 - mean_absolute_error: 0.0688 - val_loss: 0.0285 - val_mean_absolute_error: 0.1501 - lr: 0.1000\n",
      "Epoch 17/80\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0072 - mean_absolute_error: 0.0672 - val_loss: 0.0355 - val_mean_absolute_error: 0.1709 - lr: 0.1000\n",
      "Epoch 18/80\n",
      "4/4 [==============================] - 0s 53ms/step - loss: 0.0063 - mean_absolute_error: 0.0606 - val_loss: 0.0232 - val_mean_absolute_error: 0.1342 - lr: 0.1000\n",
      "Epoch 19/80\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0065 - mean_absolute_error: 0.0621 - val_loss: 0.0368 - val_mean_absolute_error: 0.1746 - lr: 0.1000\n",
      "Epoch 20/80\n",
      "4/4 [==============================] - 0s 66ms/step - loss: 0.0061 - mean_absolute_error: 0.0605 - val_loss: 0.0272 - val_mean_absolute_error: 0.1464 - lr: 0.1000\n",
      "Epoch 21/80\n",
      "4/4 [==============================] - 0s 58ms/step - loss: 0.0060 - mean_absolute_error: 0.0604 - val_loss: 0.0313 - val_mean_absolute_error: 0.1585 - lr: 0.1000\n",
      "Epoch 22/80\n",
      "4/4 [==============================] - 0s 60ms/step - loss: 0.0062 - mean_absolute_error: 0.0617 - val_loss: 0.0317 - val_mean_absolute_error: 0.1597 - lr: 0.1000\n",
      "Epoch 23/80\n",
      "4/4 [==============================] - 0s 65ms/step - loss: 0.0063 - mean_absolute_error: 0.0625 - val_loss: 0.0304 - val_mean_absolute_error: 0.1558 - lr: 0.1000\n",
      "Epoch 24/80\n",
      "4/4 [==============================] - 0s 62ms/step - loss: 0.0059 - mean_absolute_error: 0.0595 - val_loss: 0.0283 - val_mean_absolute_error: 0.1496 - lr: 0.1000\n",
      "Epoch 25/80\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0060 - mean_absolute_error: 0.0603 - val_loss: 0.0307 - val_mean_absolute_error: 0.1568 - lr: 0.1000\n",
      "Epoch 26/80\n",
      "4/4 [==============================] - 0s 52ms/step - loss: 0.0059 - mean_absolute_error: 0.0600 - val_loss: 0.0301 - val_mean_absolute_error: 0.1551 - lr: 0.1000\n",
      "Epoch 27/80\n",
      "4/4 [==============================] - 0s 57ms/step - loss: 0.0061 - mean_absolute_error: 0.0621 - val_loss: 0.0285 - val_mean_absolute_error: 0.1501 - lr: 0.1000\n",
      "Epoch 28/80\n",
      "4/4 [==============================] - 0s 54ms/step - loss: 0.0065 - mean_absolute_error: 0.0627 - val_loss: 0.0335 - val_mean_absolute_error: 0.1652 - lr: 0.1000\n",
      "Epoch 29/80\n",
      "4/4 [==============================] - 0s 51ms/step - loss: 0.0063 - mean_absolute_error: 0.0624 - val_loss: 0.0299 - val_mean_absolute_error: 0.1543 - lr: 0.1000\n",
      "Epoch 30/80\n",
      "4/4 [==============================] - 0s 61ms/step - loss: 0.0062 - mean_absolute_error: 0.0613 - val_loss: 0.0307 - val_mean_absolute_error: 0.1568 - lr: 0.1000\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "Raw predictions shape: (120, 3, 1)\n",
      "Validation data shape: (120,)\n",
      "Raw predictions shape: (120, 1)\n",
      "Reshaped predictions shape: (120, 1)\n",
      "Final predictions shape: (120,)\n",
      "Reshaped validation shape: (120, 1)\n",
      "Final validation shape: (120,)\n",
      "Fold 0 , val_loss is : 0.03, MAE scaled is : 0.16, MAE original is : 12.99\n",
      "SEQ_LEN: 3, D_MODEL: 64, INPUT_DIM: (3, 3)\n",
      "Embedded shape: (None, 3, 64)\n",
      "Positional encoding shape: (1, 3, 64)\n",
      "Epoch 1/80\n",
      "8/8 [==============================] - 8s 115ms/step - loss: 20.0704 - mean_absolute_error: 2.6894 - val_loss: 0.0995 - val_mean_absolute_error: 0.3025 - lr: 0.1000\n",
      "Epoch 2/80\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.1548 - mean_absolute_error: 0.3207 - val_loss: 0.3759 - val_mean_absolute_error: 0.6096 - lr: 0.1000\n",
      "Epoch 3/80\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.0572 - mean_absolute_error: 0.2054 - val_loss: 0.0220 - val_mean_absolute_error: 0.1266 - lr: 0.1000\n",
      "Epoch 4/80\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0319 - mean_absolute_error: 0.1467 - val_loss: 0.1533 - val_mean_absolute_error: 0.3825 - lr: 0.1000\n",
      "Epoch 5/80\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0202 - mean_absolute_error: 0.1149 - val_loss: 0.1774 - val_mean_absolute_error: 0.4132 - lr: 0.1000\n",
      "Epoch 6/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0178 - mean_absolute_error: 0.1096 - val_loss: 0.1023 - val_mean_absolute_error: 0.3072 - lr: 0.1000\n",
      "Epoch 7/80\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0134 - mean_absolute_error: 0.0951 - val_loss: 0.1078 - val_mean_absolute_error: 0.3162 - lr: 0.1000\n",
      "Epoch 8/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0133 - mean_absolute_error: 0.0967 - val_loss: 0.1106 - val_mean_absolute_error: 0.3206 - lr: 0.1000\n",
      "Epoch 9/80\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.0131 - mean_absolute_error: 0.0953 - val_loss: 0.1310 - val_mean_absolute_error: 0.3515 - lr: 0.1000\n",
      "Epoch 10/80\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0142 - mean_absolute_error: 0.1002 - val_loss: 0.1637 - val_mean_absolute_error: 0.3961 - lr: 0.1000\n",
      "Epoch 11/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0140 - mean_absolute_error: 0.0952 - val_loss: 0.1090 - val_mean_absolute_error: 0.3180 - lr: 0.1000\n",
      "Epoch 12/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0134 - mean_absolute_error: 0.0946 - val_loss: 0.1139 - val_mean_absolute_error: 0.3259 - lr: 0.1000\n",
      "Epoch 13/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0133 - mean_absolute_error: 0.0974 - val_loss: 0.0997 - val_mean_absolute_error: 0.3027 - lr: 0.1000\n",
      "Epoch 14/80\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0140 - mean_absolute_error: 0.0963 - val_loss: 0.0847 - val_mean_absolute_error: 0.2764 - lr: 0.1000\n",
      "Epoch 15/80\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0139 - mean_absolute_error: 0.0973 - val_loss: 0.0944 - val_mean_absolute_error: 0.2938 - lr: 0.1000\n",
      "Epoch 16/80\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0154 - mean_absolute_error: 0.1014 - val_loss: 0.0869 - val_mean_absolute_error: 0.2805 - lr: 0.1000\n",
      "Epoch 17/80\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0162 - mean_absolute_error: 0.1058 - val_loss: 0.1140 - val_mean_absolute_error: 0.3259 - lr: 0.1000\n",
      "Epoch 18/80\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0187 - mean_absolute_error: 0.1123 - val_loss: 0.1150 - val_mean_absolute_error: 0.3276 - lr: 0.1000\n",
      "Epoch 19/80\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0163 - mean_absolute_error: 0.1044 - val_loss: 0.1253 - val_mean_absolute_error: 0.3432 - lr: 0.1000\n",
      "Epoch 20/80\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0144 - mean_absolute_error: 0.0989 - val_loss: 0.0999 - val_mean_absolute_error: 0.3032 - lr: 0.1000\n",
      "Epoch 21/80\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.0147 - mean_absolute_error: 0.1007 - val_loss: 0.1081 - val_mean_absolute_error: 0.3167 - lr: 0.1000\n",
      "Epoch 22/80\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0137 - mean_absolute_error: 0.0977 - val_loss: 0.1328 - val_mean_absolute_error: 0.3541 - lr: 0.1000\n",
      "Epoch 23/80\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0135 - mean_absolute_error: 0.0955 - val_loss: 0.1144 - val_mean_absolute_error: 0.3267 - lr: 0.1000\n",
      "4/4 [==============================] - 0s 17ms/step\n",
      "Raw predictions shape: (120, 3, 1)\n",
      "Validation data shape: (120,)\n",
      "Raw predictions shape: (120, 1)\n",
      "Reshaped predictions shape: (120, 1)\n",
      "Final predictions shape: (120,)\n",
      "Reshaped validation shape: (120, 1)\n",
      "Final validation shape: (120,)\n",
      "Fold 1 , val_loss is : 0.11, MAE scaled is : 0.33, MAE original is : 27.24\n",
      "SEQ_LEN: 3, D_MODEL: 64, INPUT_DIM: (3, 3)\n",
      "Embedded shape: (None, 3, 64)\n",
      "Positional encoding shape: (1, 3, 64)\n",
      "Epoch 1/80\n",
      "12/12 [==============================] - 7s 64ms/step - loss: 19.0206 - mean_absolute_error: 3.0631 - val_loss: 0.6821 - val_mean_absolute_error: 0.8254 - lr: 0.1000\n",
      "Epoch 2/80\n",
      "12/12 [==============================] - 0s 42ms/step - loss: 0.6310 - mean_absolute_error: 0.6778 - val_loss: 0.1388 - val_mean_absolute_error: 0.3637 - lr: 0.1000\n",
      "Epoch 3/80\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.2901 - mean_absolute_error: 0.4603 - val_loss: 0.0187 - val_mean_absolute_error: 0.1142 - lr: 0.1000\n",
      "Epoch 4/80\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.1102 - mean_absolute_error: 0.2712 - val_loss: 0.0270 - val_mean_absolute_error: 0.1353 - lr: 0.1000\n",
      "Epoch 5/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0550 - mean_absolute_error: 0.1913 - val_loss: 0.1580 - val_mean_absolute_error: 0.3774 - lr: 0.1000\n",
      "Epoch 6/80\n",
      "12/12 [==============================] - 0s 31ms/step - loss: 0.0408 - mean_absolute_error: 0.1651 - val_loss: 0.1824 - val_mean_absolute_error: 0.4079 - lr: 0.1000\n",
      "Epoch 7/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0380 - mean_absolute_error: 0.1644 - val_loss: 0.0903 - val_mean_absolute_error: 0.2760 - lr: 0.1000\n",
      "Epoch 8/80\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0470 - mean_absolute_error: 0.1796 - val_loss: 0.1974 - val_mean_absolute_error: 0.4256 - lr: 0.1000\n",
      "Epoch 9/80\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0451 - mean_absolute_error: 0.1724 - val_loss: 0.1474 - val_mean_absolute_error: 0.3633 - lr: 0.1000\n",
      "Epoch 10/80\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0416 - mean_absolute_error: 0.1673 - val_loss: 0.0972 - val_mean_absolute_error: 0.2878 - lr: 0.1000\n",
      "Epoch 11/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0367 - mean_absolute_error: 0.1627 - val_loss: 0.1104 - val_mean_absolute_error: 0.3091 - lr: 0.1000\n",
      "Epoch 12/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0375 - mean_absolute_error: 0.1614 - val_loss: 0.1904 - val_mean_absolute_error: 0.4174 - lr: 0.1000\n",
      "Epoch 13/80\n",
      "12/12 [==============================] - 0s 39ms/step - loss: 0.0387 - mean_absolute_error: 0.1629 - val_loss: 0.1233 - val_mean_absolute_error: 0.3290 - lr: 0.1000\n",
      "Epoch 14/80\n",
      "12/12 [==============================] - 0s 35ms/step - loss: 0.0426 - mean_absolute_error: 0.1721 - val_loss: 0.2166 - val_mean_absolute_error: 0.4473 - lr: 0.1000\n",
      "Epoch 15/80\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0420 - mean_absolute_error: 0.1772 - val_loss: 0.1852 - val_mean_absolute_error: 0.4113 - lr: 0.1000\n",
      "Epoch 16/80\n",
      "12/12 [==============================] - 0s 34ms/step - loss: 0.0419 - mean_absolute_error: 0.1692 - val_loss: 0.1117 - val_mean_absolute_error: 0.3112 - lr: 0.1000\n",
      "Epoch 17/80\n",
      "12/12 [==============================] - 0s 38ms/step - loss: 0.0428 - mean_absolute_error: 0.1703 - val_loss: 0.0701 - val_mean_absolute_error: 0.2387 - lr: 0.1000\n",
      "Epoch 18/80\n",
      "12/12 [==============================] - 0s 43ms/step - loss: 0.0482 - mean_absolute_error: 0.1833 - val_loss: 0.1928 - val_mean_absolute_error: 0.4202 - lr: 0.1000\n",
      "Epoch 19/80\n",
      "12/12 [==============================] - 0s 36ms/step - loss: 0.0397 - mean_absolute_error: 0.1659 - val_loss: 0.1196 - val_mean_absolute_error: 0.3234 - lr: 0.1000\n",
      "Epoch 20/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0385 - mean_absolute_error: 0.1651 - val_loss: 0.2016 - val_mean_absolute_error: 0.4304 - lr: 0.1000\n",
      "Epoch 21/80\n",
      "12/12 [==============================] - 1s 44ms/step - loss: 0.0383 - mean_absolute_error: 0.1667 - val_loss: 0.0822 - val_mean_absolute_error: 0.2615 - lr: 0.1000\n",
      "Epoch 22/80\n",
      "12/12 [==============================] - 0s 37ms/step - loss: 0.0365 - mean_absolute_error: 0.1607 - val_loss: 0.1602 - val_mean_absolute_error: 0.3801 - lr: 0.1000\n",
      "Epoch 23/80\n",
      "12/12 [==============================] - 0s 33ms/step - loss: 0.0387 - mean_absolute_error: 0.1615 - val_loss: 0.2149 - val_mean_absolute_error: 0.4455 - lr: 0.1000\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "Raw predictions shape: (120, 3, 1)\n",
      "Validation data shape: (120,)\n",
      "Raw predictions shape: (120, 1)\n",
      "Reshaped predictions shape: (120, 1)\n",
      "Final predictions shape: (120,)\n",
      "Reshaped validation shape: (120, 1)\n",
      "Final validation shape: (120,)\n",
      "Fold 2 , val_loss is : 0.21, MAE scaled is : 0.45, MAE original is : 38.37\n"
     ]
    }
   ],
   "source": [
    "time_series_split_folds = TimeSeriesSplit(n_splits=N_SPLITS)\n",
    "performance = []\n",
    "for fold ,(training_idx, validation_idx) in enumerate(time_series_split_folds.split(X,y)):\n",
    "    transformer_model = transformer_architecture()\n",
    "\n",
    "    X_train_cv = tf.convert_to_tensor(X[training_idx], dtype=tf.float32)\n",
    "    X_val_cv = tf.convert_to_tensor(X[validation_idx], dtype=tf.float32)\n",
    "    y_train_cv = tf.convert_to_tensor(y_scaled[training_idx], dtype=tf.float32)\n",
    "    y_val_cv = tf.convert_to_tensor(y_scaled[validation_idx], dtype=tf.float32)\n",
    "    \n",
    "    transformer_model.fit(X_train_cv,y_train_cv,epochs=EPOCHS,validation_data=(X_val_cv,y_val_cv),callbacks=CALLBACK,verbose=1)\n",
    "    val_loss, val_mae = transformer_model.evaluate(X_val_cv, y_val_cv,verbose=0)\n",
    "    \n",
    "    y_val_preds = transformer_model.predict(X_val_cv)\n",
    "    y_val_preds = y_val_preds[:, -1, :]  # Shape (120, 1)\n",
    "    \n",
    "    # Reshape predictions\n",
    "    y_val_preds_reshaped = y_val_preds.reshape(-1, 1)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_preds_original = scaler.inverse_transform(y_val_preds_reshaped).flatten()\n",
    "\n",
    "    # Reshape validation data\n",
    "    y_val_reshaped = y_val_cv.numpy().reshape(-1, 1)\n",
    "\n",
    "    # Apply inverse transform\n",
    "    y_val_original = scaler.inverse_transform(y_val_reshaped).flatten() \n",
    "    \n",
    "    mae_original = np.mean(np.abs(y_val_preds_original - y_val_original))\n",
    "    \n",
    "    performance.append({\n",
    "    \"fold\": fold,\n",
    "    \"val_loss\": f'{val_loss:.4f}',\n",
    "    \"val_mae_scaled\": f'{val_mae:.4f}',\n",
    "    \"val_mae_original\": f'{mae_original:.4f}',\n",
    "    })    \n",
    "    \n",
    "    print(f'Fold {fold} , val_loss is : {val_loss:.2f}, MAE scaled is : {val_mae:.2f}, MAE original is : {mae_original:.2f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b71f67",
   "metadata": {},
   "source": [
    "Raw predictions shape: (120, 3, 1)\n",
    "Validation data shape: (120,)\n",
    "Raw predictions shape: (120, 1)\n",
    "Reshaped predictions shape: (120, 1)\n",
    "Final predictions shape: (120,)\n",
    "Reshaped validation shape: (120, 1)\n",
    "Final validation shape: (120,)\n",
    "Fold 2 , val_loss is : 0.21, MAE scaled is : 0.45, MAE original is : 38.37"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
